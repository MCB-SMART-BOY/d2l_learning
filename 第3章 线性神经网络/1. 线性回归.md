## 线性回归 (Linear Regression)

### 1. 模型与记号
对一个样本 $x \in \mathbb{R}^d$，线性回归写成:
$$
\hat{y} = w^\top x + b
$$
- $w$: 权重向量 (weights)
- $b$: 偏置 (bias)
- 数据集: $X\in\mathbb{R}^{n\times d}$, $y\in\mathbb{R}^n$

```python
# 批量写法(向量化)
# X: (n, d), w: (d, 1), b: (1, )
y_hat = X @ w + b  # (n, 1)
```

---

### 2. 损失函数: 平方损失 (L2)
单样本损失:
$$
l(y, \hat{y}) = \frac{1}{2}(\hat{y} - y)^2
$$
整体经验风险:
$$
L = \frac{1}{n}\sum_{i=1}^n l(y_i, \hat{y}_i)
$$
**多写一个 1/2** 的意义: 求导结果更好看

---

### 3. 最小二乘(正规方程)只当了解
$$
w^* = (X^\top X)^{-1}X^\top y
$$
- 前提是 $X^\top X$ 可逆
- 但真实数据通常很大，算矩阵逆成本高
- 所以实践里基本都是用梯度下降(含小批量)

---

### 4. 梯度(写出来心里更踏实)
对单样本:
$$
\frac{\partial l}{\partial w} = (\hat{y}-y)x, \quad
\frac{\partial l}{\partial b} = \hat{y}-y
$$
这就是 SGD 更新公式的来源

---

### 5. 线性回归 = 单层神经网络
把 $x$ 喂给线性层, **不加激活函数**，输出一个标量
