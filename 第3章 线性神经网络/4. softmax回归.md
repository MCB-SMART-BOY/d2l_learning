## softmax 回归 (多分类的线性模型)

### 1. 线性回归 vs 分类问题
- 回归: $y$ 是连续数值
- 分类: $y$ 是离散类别 $\{1,2,\dots,q\}$

目标: 输出“每个类别的概率”

---

### 2. 模型输出 (先线性, 再 softmax)
对每个样本:
$$
\mathbf{o} = W^\top x + b \quad (\mathbf{o}\in\mathbb{R}^q)
$$
softmax 把 $\mathbf{o}$ 变成概率:
$$
\hat{y}_j = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)}
$$

```python
# X: (n, d), W: (d, q), b: (q, )
O = X @ W + b  # (n, q) 这一步叫 logits(还没转概率)
Y_hat = torch.softmax(O, dim = 1)  # dim = 1 表示对每一行(每个样本)做归一化
```

---

### 3. 交叉熵损失 (Cross-Entropy)
如果用 one-hot 标签 $y$:
$$
\ell(y, \hat{y}) = -\sum_{j=1}^q y_j \log \hat{y}_j
$$
实际更常见写法:
$$
\ell(y, \hat{y}) = -\log \hat{y}_{y}
$$

> 直觉: 你预测对的概率越大, 损失越小

---

### 4. 预测与准确率
- 预测类别: $\arg\max_j \hat{y}_j$
- 准确率: 预测对的比例

```python
pred = Y_hat.argmax(dim = 1)        # 每行取最大概率的类别
acc = (pred == y).float().mean()    # True/False -> 1/0，然后求平均
```

---

### 5. 数值稳定性 (小坑)
softmax 可能会爆成 `inf`, 一个常见技巧:
$$
\text{softmax}(o) = \frac{\exp(o - \max(o))}{\sum_k \exp(o_k - \max(o))}
$$

> 框架里的 `CrossEntropyLoss` 已经把 log-softmax + 稳定性处理打包好了
