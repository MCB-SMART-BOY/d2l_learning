真佛了 框架版本少写一堆细节
### 1. 造数据(和从零开始一样)
```python
import torch
from torch.utils import data  # DataLoader 在这

true_w = torch.tensor([2.0, -3.4])
true_b = 4.2
num_examples = 1000

X = torch.normal(0, 1, (num_examples, 2))
noise = torch.normal(0, 0.01, (num_examples, ))  # 加点噪声更像真实数据
y = (X @ true_w + true_b + noise).reshape(-1, 1)  # 统一成 (n, 1)

batch_size = 10

data_iter = data.DataLoader(
    data.TensorDataset(X, y),  # 把特征和标签绑一起
    batch_size = batch_size,
    shuffle = True,            # 每轮打乱更稳
)
```

---

### 2. 定义模型 (nn.Linear)
```python
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))  # 一个线性层 = 线性回归

# 初始化权重 & 偏置 (不写也有默认初始化，但我习惯手动)
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

---

### 3. 损失函数 + 优化器
```python
loss = nn.MSELoss()  # 默认是 mean
trainer = torch.optim.SGD(net.parameters(), lr = 0.03)  # 经典 SGD
```

---

### 4. 训练
```python
num_epochs = 3

for epoch in range(num_epochs):
    for X_batch, y_batch in data_iter:
        l = loss(net(X_batch), y_batch)
        trainer.zero_grad()  # 不清零会梯度累积
        l.backward()
        trainer.step()

    train_l = loss(net(X), y)
    print(f"epoch {epoch + 1}, loss {train_l:.6f}")

print(net[0].weight.data.reshape(true_w.shape))
print(net[0].bias.data)
```
