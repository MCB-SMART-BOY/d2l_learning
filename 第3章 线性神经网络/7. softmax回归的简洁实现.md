### 1. 定义模型
```python
import torch
from torch import nn

net = nn.Sequential(
    nn.Flatten(),          # (batch, 1, 28, 28) -> (batch, 784)
    nn.Linear(784, 10),    # 线性层输出 10 类 logits
)
```

---

### 2. 损失 + 优化
```python
loss = nn.CrossEntropyLoss()  # 自带 softmax + log
trainer = torch.optim.SGD(net.parameters(), lr = 0.1)  # 还是最朴素的 SGD
```

> `train_iter` / `test_iter` 的定义见上一节数据集部分

---

### 3. 训练
```python
def accuracy(y_hat, y):
    pred = y_hat.argmax(dim = 1)  # 取最大 logit 的类别
    return (pred == y).float().sum()

@torch.no_grad()
def evaluate_accuracy(net, data_iter):
    metric = 0.0      # 对了多少
    num_examples = 0  # 总共多少
    for X, y in data_iter:
        metric += accuracy(net(X), y)
        num_examples += y.numel()
    return metric / num_examples

num_epochs = 10

for epoch in range(num_epochs):
    for X, y in train_iter:
        y_hat = net(X)           # 输出 logits，不要手动 softmax
        l = loss(y_hat, y)       # y 直接给类别索引就行
        trainer.zero_grad()      # 不清零会梯度累积
        l.backward()
        trainer.step()

    train_acc = evaluate_accuracy(net, train_iter)
    test_acc = evaluate_accuracy(net, test_iter)
    print(f"epoch {epoch + 1}: train_acc {train_acc:.3f}, test_acc {test_acc:.3f}")
```

### 4. 小结
- `CrossEntropyLoss` = `LogSoftmax` + `NLLLoss`
- 所以这里不要手动 `softmax`，交给框架处理
