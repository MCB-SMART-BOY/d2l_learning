### 1. requires_grad: 声明这个张量需要求导
```python
import torch

X = torch.arange(4.0, requires_grad = True)
Y = (X * X).sum()  # 标量

Y.backward()
X.grad  # dy/dx = 2x
````

---
### 2. 梯度会累积(不自动清零)——大坑

```python
X = torch.tensor([1.0, 2.0, 3.0], requires_grad = True)

X.sum().backward()
X.grad  # [1,1,1]

(2*X).sum().backward()
X.grad  # [3,3,3] 发生了累积！
```

> 训练循环里必须手动清零: `optimizer.zero_grad()` 或 `X.grad.zero_()` 
---
### 3. 非标量怎么 backward？

```python
X = torch.arange(4.0, requires_grad = True)
Y = X * X  # shape=(4, )

Y.sum().backward()  # 方式1: 先变标量
X.grad.zero_()      # 养成清理梯度的好习惯
# 或者: 
# Y.backward(torch.ones_like(Y))  # 方式2: 传入外部(并非)梯度
```
---
### 4. detach / no_grad: 我不想让它求导

```python
X = torch.randn(3, requires_grad=True)

Y = X.detach()     # 切断梯度历史(但可能共享数据)
X.detach().item()  # 我觉得不错的写法 可以用来提取信息并且不被记录
with torch.no_grad():
    Z = X * 2      # 这段里完全不记录计算图(推理常用)
```

---

### 5. 原地操作: 可能会把反传搞炸

```python
X = torch.randn(3, requires_grad = True)
Y = X * 2
# X.add_(1)  # 可能导致 backward 报错或梯度异常
```
