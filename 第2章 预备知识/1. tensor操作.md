## 深度了解 PyTorch 语法
其中最重要的就是 tensor张量 可以理解为 多维数组 支持 GPU 并行计算
并且这个类除了数值以外还有 dtype/device/grad 等属性(后面会用到)
接下来所有预备知识, 绝大部分都是对 Tensor 做操作(就是需要点数学基础)

---

### 1. 数据生成

```python
import torch

# 1) 直接生成一个指定每个位置数值的 2 x 3 张量
X_2_3 = torch.tensor([
  [0, 1, 2],
  [3, 4, 5],
])

# 2) 生成一个从 0 开始到 n - 1 的 1 维张量
# 相当于 X_6 = torch.tensor([0, 1, 2, 3, 4, 5])
X_arange_6 = torch.arange(6)

# 3) 生成值全是 0 的 2 x 3 张量(注意: 参数是元组哈)
X_zeros_2_3 = torch.zeros((2, 3))

# 4) 生成值全是 1 的 2 x 3 张量
X_ones_2_3 = torch.ones((2, 3))

# 5) 生成一个 2 行 3 列随机数值张量
# randn: 每个元素服从标准正态分布 X ~ N(0, 1)
# rand: 那就服从均匀分布 X ~ U(0, 1) 了
X_randn_2_3 = torch.randn(2, 3)

# 6) 生成与 X_2_3 相同形状的张量, 但全是 0
X_like_zeros = torch.zeros_like(X_2_3)

# 7) 生成与 X_2_3 相同形状的张量, 但全是 1
X_like_ones = torch.ones_like(X_2_3)
````

---

### 2. 查看数据属性(Attributes / Methods)

```python
# 1) 张量中一共有多少个值(numbers of elements?)
X.numel()

# 2) 张量形状(注意: 这是属性, 不是方法)
X.shape

# 3) 维度数(几个轴)
X.ndim

# 4) dtype: 数据类型(int64/float32 等)
X.dtype

# 5) device: 在 CPU 还是 GPU 上(默认 CPU)
X.device
```

---

### 3. 操作数据属性(reshape / 转置 / 换轴)

```python
X = torch.arange(12)  # shape: (12, )

# 1) reshape: 只要元素总数不变, 就能换形状
X_3_4 = X.reshape(3, 4)  # shape: (3, 4)

# 2) -1 自动推断(前提: 能整除)
X_auto = X.reshape(-1, 4)  # 这里会推断成 (3, 4)

# 3) 转置(2D)
Xt = X_3_4.T  # shape: (4, 3)

# 4) 多维换轴(3D 以上用 permute)
Y_2_3_4 = torch.randn(2, 3, 4)     # shape: (2, 3, 4)
Y_perm = Y_2_3_4.permute(2, 0, 1)  # shape: (4, 2, 3)

# 5) 这里就要补充一个点了
# 在 reshape 之后会出现不连续内存的情况
X_3_4_contiguous = X_3_4.contiguous()  # 这样就可以对齐成连续内存
# 使用 contiguous 很可能会出现大量复制
# 所以写成这样应该不是一个好习惯
X_3_4_contiguous = torch.arange(12).reshape(3, 4).contiguous()
```

---

### 4. 操作数据的方法(索引 / 切片 / 拼接 / 堆叠 / 广播前置)

#### 4.1 索引切片(取数据)

```python
X = torch.arange(12).reshape(3, 4)

X[0]     # 第 0 行 -> shape: (4, )
X[:, 1]  # 第 1 列 -> shape: (3, )
X[1, 2]  # 单个元素 -> 一个0维tensor(还是 tensor, 不是 Python 数)
```

#### 4.2 赋值(改数据)

```python
X = torch.arange(12).reshape(3, 4)

X[:2, :] = 12  # 把前 2 行全改成 12(原地修改)
```

#### 4.3 拼接(cat)与堆叠(stack)

```python
A = torch.ones((2, 3))
B = torch.zeros((2, 3))

torch.cat([A, B], dim = 0)    # 沿行拼 -> shape: (4, 3)
torch.cat([A, B], dim = 1)    # 沿列拼 -> shape: (2, 6)

torch.stack([A, B], dim = 0)  # 新增一个维度 -> shape: (2, 2, 3)
```

---

### 5. 数据计算的方法(按元素 / 归约 / 广播 / 比较)

#### 5.1 按元素运算(elementwise)

对于同 shape 的张量 X, Y:   
$$
(X \circ Y)_{ij} = X_{ij} \circ Y_{ij}  
$$

```python
X = torch.tensor([
    [1., 2.],
    [3., 4.],
])
Y = torch.tensor([
    [10., 20.],
    [30., 40.],
])

X + Y, X - Y, X * Y, X / Y, X ** 2
```

#### 5.2 按元素函数(exp 等)

$$ Y_{ij} = e ^ {X_{ij}} $$  
```python
Y = torch.exp(X)
```

#### 5.3 比较(返回 bool 张量)

```python
X = torch.ones((2, 3))
Y = torch.zeros_like(X)

X == Y
# 输出是(不按照我喜好来的写法):
# tensor([[False, False, False],
#         [False, False, False]])
```

#### 5.4 归约运算(sum/mean/max: 按维聚合)

```python
X = torch.arange(12).reshape(3, 4)

X.sum()                         # -> 标量(0-dim tensor)
X.sum(dim = 0)                  # 沿行方向聚合 -> shape: (4, )
X.sum(dim = 1)                  # 沿列方向聚合 -> shape: (3, )

X.sum(dim = 1, keepdim = True)  # -> shape: (3, 1) 方便后续广播
```

#### 5.5 广播(Broadcasting)

很多操作支持“广播”: 当 shape 不一样但能对齐时, PyTorch 会自动扩展
大概数学表达张这样:

$$
A \in\mathbb {R} ^ {3 \times 1}, \ B \in\mathbb {R} ^ {1 \times 2} \Rightarrow A + B \in\mathbb {R} ^ {3\times 2}  
$$

```python
A = torch.arange(3).reshape(3, 1)  # (3,1)
B = torch.arange(2).reshape(1, 2)  # (1,2)
A + B                              # (3,2)
```

---

### 6. 节省内存

很多表达式会创建新张量 如果想原地更新, 用 `X[:] = ...` 或 `X += ...` 

```python
X = torch.ones((2, 3))
Y = torch.ones((2, 3))

# 1) 产生新张量(会新开内存)
Z = X + Y

# 2) 原地写回(更省内存)
X[:] = X + Y

# 3) 更短: 就地加(in-place-ish), 也常见
X += Y
```

---

### 7. Tensor 与 Python / NumPy 互转

```python
import numpy as np

X = torch.arange(6).reshape(2, 3)

A_np = X.numpy()  # Tensor -> numpy(很多时候共享内存)
B_torch = torch.from_numpy(A_np)  # numpy -> Tensor(也可能共享内存)

s = torch.tensor([3.5])
s.item()  # Tensor -> Python 标量
```
