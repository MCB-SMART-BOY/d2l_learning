## 0. 事件 vs 随机变量（新手常糊）
- **事件 (event)**: 一个 发生/不发生 的东西, 比如"骰子掷出 1"
- **随机变量 (random variable)**: 把“结果”映射成数字/类别的东西, 比如用$X\in\{1,2,3,4,5,6\}$表示骰子点数
## 1. 抽样（sampling）与分布（distribution）
书里的说法很直接：
从分布里 抽取样本 的过程叫 **抽样**
把概率分配给离散选择的分布叫 **多项分布（multinomial distribution）**
### 1.1 Multinomial：一次/多次抽样的“计数结果”
```python
import torch
from torch.distributions import Multinomial

# 一个公平骰子: 6 个结果概率都一样
probs = torch.ones(6) / 6

# 抽 1 次 (total_count = 1), 返回长度 6 的“计数向量”
# 哪个位置是 1 就表示抽到了哪个点数
one = Multinomial(1, probs = probs).sample()
one

# 抽 10 次, 返回每个点数出现次数 (总和 = 10)
ten = Multinomial(10, probs = probs).sample()
ten, ten.sum()
```

**易错点：**

- `Multinomial(n, probs).sample()` 返回的是计数（counts），不是直接返回类别索引
- 计数的总和应该等于 `total_count`(例如 10) 如果看到不是，先检查是不是自己把维度/批量搞错了
---
## 2. 大数定律：用频率逼近概率
投掷次数越多, 用“出现次数 / 总次数”估计的概率会越来越接近真实概率 (**大数定律**)

### 2.1 多组实验 + 累计频率（D2L 的典型写法）

```python
import torch
from torch.distributions import Multinomial

probs = torch.ones(6) / 6

# 500 组实验，每组掷 10 次骰子
counts = Multinomial(10, probs=probs).sample((500, ))  # (500, 6)

# 前 i 组的累计计数
cum_counts = counts.cumsum(dim = 0)                     # (500, 6)

# 估计概率 = 累计计数 / 累计总次数
# 顺便一提 pandas 上是 keepdims 所以容易写错
estimates = cum_counts / cum_counts.sum(dim = 1, keepdim = True)  # (500, 6)
estimates[-1]   # 看最后一组累计后的估计
```
---
## 3. 联合概率（joint probability）

单事件里写 $P(A)$ ; 多变量时要写联合: 

$$P(A=a,;B=b)$$

准确的说叫: **"A 取 a 并且 B 取 b 同时发生的概率是多少？"**  
并且书里提醒了一个很直观的不等式：

$$P(A=a,;B=b)\le P(A=a)$$

因为“同时发生”一定不比“只发生一个”更容易

---

## 4. 条件概率（conditional probability）（你已有，补一个关键推导）

你写得很标准：

$$P(B|A)=\frac{P(A\cap B)}{P(A)}\quad (P(A)>0)$$

把它稍微变形一下，就得到**乘法法则（multiplication rule）**：

$$P(A\cap B)=P(B|A)P(A)$$

这个等式在 D2L 里是推贝叶斯公式的起点
## 5. 贝叶斯公式
同样标准：
理解方式: 
- (P(A))：先验（prior）——你一开始觉得 A 多常见
- (P(B|A))：似然（likelihood）——如果 A 真的发生，看到 B 的概率
- (P(A|B))：后验（posterior）——看见 B 之后，你更新对 A 的相关程度
---
## 6. 边际化(marginalization)

求和法则(sum rule)
$$P(B)=\sum_A P(A_i B)$$
得到的 $P(B)$ 叫边际概率/边际分布
---
## 7. 独立性(independence)
D2L 的定义要点：

- 写作: $A\perp B$
- 等价条件(之一):
$$P(A|B)=P(A)$$
- 以及: 
$$A\perp B \iff P(A\cap B)=P(A)P(B)$$
### 注: 独立 and 互斥（互不相容）
- **互斥**: $A\cap B=\varnothing\Rightarrow P(A\cap B)=0$    
- **独立**: $P(A\cap B)=P(A)P(B)$
如果 (A,B) 都不是 0 概率事件，通常 (P(A)P(B)>0)，所以互斥一般**不可能独立**。  
---
## 8. HIV 检测
不如直接看书
---
## 9. torch.multinomial：按权重抽样
输入必须**非负、有限、和不为 0**
并且不要求归一化为 1(当权重用)
### 9.1 常见用法与坑

```python
import torch

w = torch.tensor([1.0, 3.0, 6.0])   # 权重，不必和为 1
idx = torch.multinomial(w, num_samples = 5, replacement = True)
idx
```

**再补两个坑：**
1. `replacement = False` 时，`num_samples` 不能超过类别数，否则报错
2. 如果 `input` 是二维 `(m, n)`，它会对每一行分别抽样，输出 shape 是 `(m, num_samples)`
---
## 10. 期望（expectation / average）

D2L 的定义 (离散情况): 
$$E[X]=\sum_x x;P(X=x)$$
以及更通用的:
$$E_{x\sim P}[f(x)]=\sum_x f(x);P(x)$$
### 10.1 用公平骰子举例
令$X\in{1,2,3,4,5,6}$, 且 $P(X=i)=\frac16$，那么  
$$  
E[X]=\sum_{i=1}^6 i\cdot\frac16=\frac{1+2+3+4+5+6}{6}=3.5  
$$

### 10.2 代码写法
**(1) 用概率表直接算（不抽样）**
```python
import torch
vals = torch.arange(1, 7, dtype=torch.float32)
probs = torch.ones(6) / 6
EX = (vals * probs).sum()
EX
```
**(2) 用抽样近似（会有噪声，但样本多就稳）**
```python
import torch
idx = torch.multinomial(torch.ones(6), 100000, replacement=True)  # 抽很多次
samples = (idx + 1).float()  # idx 是 0~5，转成 1~6
samples.mean()
```
---
## 11. 方差（variance）与标准差（standard deviation）
D2L 给的核心公式:
$$  
\mathrm{Var}[X]=E[(X-E[X])^2]=E[X^2]-E[X]^2  
$$
标准差就是:  
$$\mathrm{Std}[X]=\sqrt{\mathrm{Var}[X]}$$
并且函数的方差也类似:
$$  
\mathrm{Var}[f(x)] = E\left[\left(f(x)-E[f(x)]\right)^2\right]  
$$
### 11.1 PyTorch 里 var 的一个大坑
`torch.var` 用 `correction` 参数（以前叫 `unbiased`）。  
并且默认是 **Bessel 校正**：`correction=1`（更像“样本方差”），不是 “总体方差”。
```python
import torch
x = torch.tensor([1., 2., 3., 4.])

x.var(correction=0)  # 总体方差（除以 N）
x.var(correction=1)  # 样本方差（除以 N-1），这是默认值
```